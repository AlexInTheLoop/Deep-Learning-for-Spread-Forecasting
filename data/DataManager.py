import os
import pandas as pd
import numpy as np
import requests
import zipfile
import tensorflow as tf
from sklearn.model_selection import train_test_split

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.daily_features_builders import (
get_serial_dependancy_features_v2,
normalize
)
from utils.res_comp import get_parametric_estimators_series

RAW_DATA_DIR = "raw data"
LABELS_DATA_DIR = "labels data"
DAILY_FEATURES_DATA_DIR = "daily features data"
MINUTE_FEATURES_DATA_DIR = "minute features data"
PARAMETRIC_ESTIMATORS_DIR = "parametric estimators"

LABELS_LABEL = "labels"
FEATURES_LABEL = "features"

class DataManager:
    """
    Classe permettant de r√©cup√©rer les donn√©es et de pr√©parer construire les features pour les mod√®les

    Arguments :
    - symbols : liste contenant un ensemble de cryptos (pour le train ou le test)
    - dates : liste contenant un ensemble de dates (pour le train ou le test)
    - light : bool√©en pour all√©ger l'ouverture des fichiers
    """
    def __init__(self, symbols: list, dates:list,light = False):
        self.symbols:list = [s.upper() for s in symbols]
        self.dates: list = dates
        self.freq:str = "1m"
        self.nb_assets:int = len(self.symbols)
        self.light = light

        # R√©cup√©ration des diff√©rents paths vers les r√©pertoires o√π seront stock√©es les features, les labels, ...
        self.module_dir = os.path.dirname(os.path.abspath(__file__))
        self.raw_data_dir = os.path.join(self.module_dir, RAW_DATA_DIR)
        self.labels_data_dir = os.path.join(self.module_dir, LABELS_DATA_DIR)
        self.minute_features_data_dir = os.path.join(self.module_dir, MINUTE_FEATURES_DATA_DIR)
        self.parametric_estimators_dir = os.path.join(self.module_dir, PARAMETRIC_ESTIMATORS_DIR)
        os.makedirs(self.raw_data_dir, exist_ok=True)
        os.makedirs(self.labels_data_dir, exist_ok=True)
        os.makedirs(self.minute_features_data_dir, exist_ok=True)
        os.makedirs(self.parametric_estimators_dir, exist_ok=True)

        # R√©cup√©ration des url pour t√©l√©charger les barres d'une minute et les carnets d'ordres
        self.kline_base_url = "https://data.binance.vision/data/futures/um/monthly/klines"
        self.bookticker_base_url = "https://data.binance.vision/data/futures/um/monthly/bookTicker"

    def download_and_prepare_data(self):
        """
        M√©thode central de t√©l√©charger les barres d'une minute et les donn√©es de carnet d'ordres
        """

        # Boucle par actif
        for symbol in self.symbols:
            # Boucle par date
            for year, month in self.dates:

                month_str = f"{month:02d}"
                year_str = str(year)

                base_name = f"{symbol}-bookTicker-{year_str}-{month_str}"
                label_path = os.path.join(self.labels_data_dir, f"{base_name}_labels.parquet")

                # Premi√®re partie : import des barres d'une minute et construction des features
                kline_filename = f"{symbol}-{self.freq}-{year_str}-{month_str}"
                kline_parquet_path = os.path.join(self.raw_data_dir, f"{kline_filename}.parquet")

                # Si les donn√©es n'ont pas d√©j√† √©t√© import√©es, on les importe
                if not os.path.exists(kline_parquet_path):
                    kline_zip_url = f"{self.kline_base_url}/{symbol}/{self.freq}/{kline_filename}.zip"
                    kline_zip_path = os.path.join(self.raw_data_dir, f"{kline_filename}.zip")
                    print(f"T√©l√©chargement Klines : {kline_zip_url}")
                    response = requests.get(kline_zip_url)

                    # En l'absence d'erreur, d√©zipage des donn√©es
                    if response.status_code != 200:
                        raise Exception(f"Erreur t√©l√©chargement : {kline_zip_url}")
                    with open(kline_zip_path, "wb") as f:
                        f.write(response.content)
                    with zipfile.ZipFile(kline_zip_path, 'r') as zip_ref:
                        zip_ref.extractall(self.raw_data_dir)
                    os.remove(kline_zip_path)

                    # Ouverture du fichier CSV et sauvegarde en parquet
                    csv_path = os.path.join(self.raw_data_dir, f"{kline_filename}.csv")
                    df_kline = pd.read_csv(csv_path)
                    df_kline.columns = [
                        "open_time", "open", "high", "low", "close", "volume",
                        "close_time", "quote_asset_volume", "nb_trades",
                        "taker_buy_base", "taker_buy_quote", "ignore"
                    ]
                    df_kline.to_parquet(kline_parquet_path, index=False)

                    # Destruction du CSV
                    os.remove(csv_path)
                    print(f"Klines sauvegard√© : {kline_parquet_path}")
                else:
                    print(f"Klines d√©j√† existant : {kline_parquet_path}")

                # Deuxi√®me partie : import des orders books
                bookticker_filename = f"{symbol}-bookTicker-{year_str}-{month_str}"
                bookticker_zip_filename = f"{bookticker_filename}.zip"
                bookticker_csv_filename = f"{bookticker_filename}.csv"

                bookticker_zip_url = f"{self.bookticker_base_url}/{symbol}/{bookticker_zip_filename}"

                bookticker_zip_path = os.path.join(self.raw_data_dir, bookticker_zip_filename)
                csv_path = os.path.join(self.raw_data_dir, bookticker_csv_filename)
                bookticker_parquet_path = os.path.join(self.raw_data_dir, f"{bookticker_filename}.parquet")

                # Si les donn√©es n'ont pas d√©j√† √©t√© import√©es, elles sont import√©es √† nouveau
                if not os.path.exists(bookticker_parquet_path) and not os.path.exists(label_path):
                    print(f"T√©l√©chargement BookTicker : {bookticker_zip_url}")
                    response = requests.get(bookticker_zip_url, stream=True)

                    if response.status_code != 200:
                        print(f"BookTicker indisponible : {bookticker_zip_url} (code {response.status_code})")
                        continue

                    with open(bookticker_zip_path, "wb") as f:
                        f.write(response.content)

                    with zipfile.ZipFile(bookticker_zip_path, 'r') as zip_ref:
                        zip_ref.extractall(self.raw_data_dir)
                    os.remove(bookticker_zip_path)

                    # Pour acc√©lerer les traitements, seules les colonnes requises pour le calcul du spread journalier sont conserv√©es
                    if self.light:
                        reader = pd.read_csv(
                        csv_path,
                        usecols=["transaction_time", "best_bid_price", "best_ask_price"],
                        dtype={
                            "transaction_time": np.int64,
                            "best_bid_price": np.float32,
                            "best_ask_price": np.float32
                        },
                        chunksize=1_000_000
                        )
                        df_bt = pd.concat(reader, ignore_index=True)
                    else:
                        df_bt = pd.read_csv(csv_path)

                    # V√©rification sur le nombre de lignes
                    n_rows = len(df_bt)
                    if n_rows < 10_000_000:
                        print(f"Attention : fichier bookTicker anormalement petit ({n_rows} lignes)")

                    # Sauvegarde en parquet et destruction du CSV
                    df_bt.to_parquet(bookticker_parquet_path, index=False)
                    os.remove(csv_path)
                    print(f"BookTicker sauvegard√© : {bookticker_parquet_path}")
                else:
                    print(f"BookTicker d√©j√† existant : {bookticker_parquet_path}")

    def load_features(self, serial_dependency:bool = False, clean_features: bool = False, use_tick_size: bool = False):
        """
        M√©thode permettant de sauvegarder les features intra-day utilis√©es pour estimer les mod√®les.

        Arguments :
        - serial_dependancy: bool√©en pour d√©terminer s'il faut utiliser des features de d√©pendance s√©rielle ou non
        - do_aggregate : bool√©en pour d√©terminer s'il faut agr√©ger les donn√©es de la s√©quence (MLP, ...)
        """
        processed_paths = []

        # Retraitement sur les jours pour garantir l'uniformisation des donn√©es
        nb_days:int = 30
        nb_minute_per_day:int = 1440
        nb_sequences:int = nb_days * nb_minute_per_day

        # Cas o√π l'utilisateur souhaite supprimer les features cr√©√©es pr√©c√©demment
        if clean_features:
            self._clean_features("labels")

        # Double boucle par actif et date
        for symbol in self.symbols:
            for year, month in self.dates:
                month_str = f"{month:02d}"
                filename = f"{symbol}-{self.freq}-{year}-{month_str}"
                parquet_path = os.path.join(self.raw_data_dir, f"{filename}.parquet")

                # R√©cup√©ration du chemin vers les features en minutes
                processed_path = os.path.join(
                    self.minute_features_data_dir,
                    f"{filename}_features_1min_klines.parquet"
                )

                # Si le fichier existe d√©j√†, pas besoin de refaire les calculs, sinon on construit les features
                if os.path.exists(processed_path):
                    print(f"Fichier d√©j√† existant, ignor√© : {processed_path}")
                    processed_paths.append(processed_path)
                    continue
                print(f"Construction des features pour : {symbol} {year}-{month_str}")

                # Construction des features intraday avec / sans d√©pendance s√©rielle
                df_out = self.build_features(parquet_path, use_serial_dependency=serial_dependency)

                # Cas o√π l'utilisateur souhaite utiliser le ticksize comme feature
                if use_tick_size:
                    tick_size: float = self._load_ticksize(symbol)
                    df_out["ticksize"] = tick_size

                # Si on travaille sur un mois √† moins de 30 jours, on passe (uniformisation des donn√©es) ==> √† revoir ; cas √† +30 jours g√©r√©s dans la m√©thode
                if df_out.shape[0] < nb_sequences:
                    continue

                # Cas d'un mois > 30 jours : seuls les 30 premiers jours sont conserv√©s
                if df_out.shape[0] > nb_sequences:
                    df_out = df_out.iloc[0:nb_sequences, :]

                # Sauvegarde
                df_out.to_parquet(processed_path, index=False)
                processed_paths.append(processed_path)
                print(f"Features g√©n√©r√©es pour l'actif {symbol} et la date {year}-{month_str}")
        return processed_paths

    def _clean_features(self, elem_model:str):
        """
        M√©thode permettant de supprimer tous les fichiers contenant les features / labels cr√©√©s lors des
        run pr√©c√©dents
        """
        print(f"Suppression des fichiers de {elem_model} cr√©√©s pr√©c√©demment")
        if elem_model == LABELS_LABEL:
            filelist: list = [f for f in os.listdir(self.labels_data_dir)]
        elif elem_model == FEATURES_LABEL:
            filelist: list = [f for f in os.listdir(self.minute_features_data_dir)]
        else:
            raise Exception(f"Aucun r√©pertoire n'est associ√© √† {elem_model}")

        # Boucle sur tous les fichiers et suppression
        for f in filelist:
            if elem_model == LABELS_LABEL:
                # R√©cup√©ration du path absolu du fichier
                file_path = os.path.join(self.labels_data_dir, f)
            else:
                file_path = os.path.join(self.minute_features_data_dir, f)
            os.remove(file_path)
        print("Suppression des fichiers de features termin√©e")

    @staticmethod
    def _load_ticksize(symbol: str)->float:
        """
        M√©thode permettant de t√©l√©charger le ticksize associ√© au ticker
        pour tester son pouvoir pr√©dictif dans le mod√®le de Deep Learning
        """

        # Cr√©ation du ticksize (nan par d√©faut)
        tick_size: float = np.nan
        try:
            # Import du ticksize depuis l'API binance
            exchange_info = requests.get(
                f"https://fapi.binance.com/fapi/v1/exchangeInfo?symbol={symbol}"
            ).json()
            for f in exchange_info["symbols"][0]["filters"]:
                if f["filterType"] == "PRICE_FILTER":
                    tick_size = float(f["tickSize"])
                    break
        except Exception as e:
            print(f"Erreur r√©cup√©ration tick_size : {e}")

        # R√©cup√©ration du ticksize
        return tick_size

    @staticmethod
    def build_features(klines_path, use_serial_dependency: bool = False):
        """
        M√©thode permettant de construire les features intraday

        Les features intraday utilis√©es sont :
        - OHCL + Vol
        - Les rendements entre deux minutes
        - Le spread H-L au cours d'une minute
        - L'√©volution du spread H-L
        - La volatilit√© des rendements
        - L'√©volution du volume
        - Le volume moyen sur une fen√™tre pass√©e
        - Le jour de la semaine
        - Le num√©ro de l'actif (= pseudo encodage ==> √† voir)
        """

        # Ouverture du fichier
        df = pd.read_parquet(klines_path)

        # Cr√©ation d'un dataframe pour stocker les features
        df_features: pd.DataFrame = pd.DataFrame()

        # R√©cup√©ration de la date au format datetime
        df_features["datetime"] = pd.to_datetime(df["open_time"], unit="ms")
        df_features["date"] = df_features["datetime"].dt.date

        # R√©cup√©ration des barres + volume
        df_features["close"] = df["close"].astype(float)
        df_features["open"] = df["open"].astype(float)
        df_features["high"] = df["high"].astype(float)
        df_features["low"] = df["low"].astype(float)
        df_features["volume"] = df["volume"].astype(float)

        # Calcul du rendement et de la vol sur 10 minutes
        df_features["returns"] = df_features["close"].pct_change().fillna(0)
        df_features["volatility"] = df_features["returns"].rolling(window=10, min_periods=1).std().fillna(0)

        # Calcul de l'√©volution du volume / moyenne du volume sur les 10 minutes pass√©es
        df_features["volume_change"] = df_features["volume"].pct_change().fillna(0)
        df_features["rolling_mean_volume"] = df_features["volume"].rolling(window=10, min_periods=1).mean().fillna(df_features["volume"])

        # Calcul du spread High - Low du jour et de son √©volution entre deux dates (en VA)
        df_features["spread_high_low"] = df_features["high"] - df_features["low"]
        df_features["delta_spread_high_low"] = np.abs(df_features["spread_high_low"].pct_change().fillna(0))

        # R√©cup√©ration des informations temporelles
        df_features["year"] = df_features["datetime"].dt.year
        df_features["month"] = df_features["datetime"].dt.month
        df_features["day"] = df_features["datetime"].dt.day
        df_features["hour"] = df_features["datetime"].dt.hour

        # Int√©gration √©ventuelle d'indicateurs de d√©pendance s√©rielle
        if use_serial_dependency:

            # R√©cup√©ration du dataframe avec d√©pendance s√©rielle
            df_features = get_serial_dependancy_features_v2(df_features)


        # Suppression de la colonne de dates
        df_features.drop("datetime", axis=1, inplace=True)
        df_features.drop("date", axis=1, inplace=True)

        # r√©cup√©ration du dataframe
        return df_features

    def build_labels(self, clean_labels: bool = False)->list:
        """
        M√©thode permettant de calculer les spreads journaliers
        """
        os.makedirs(self.labels_data_dir, exist_ok=True)

        label_paths = []

        # Retraitement sur les jours pour garantir l'uniformisation des donn√©es
        nb_days:int = 30

        # Cas o√π l'utilisateur souhaite supprimer les labels cr√©√©s pr√©c√©demment
        if clean_labels:
            self._clean_features("labels")

        # Double boucle actif / date
        for symbol in self.symbols:
            for (year, month) in self.dates:
                month_str = f"{month:02d}"
                year_str = str(year)
                base_name = f"{symbol}-bookTicker-{year_str}-{month_str}"
                parquet_path = os.path.join(self.raw_data_dir, f"{base_name}.parquet")
                label_path = os.path.join(self.labels_data_dir, f"{base_name}_labels.parquet")

                # V√©rification de l'existence des fichiers (s'ils n'existent pas, on r√©alise les calculs)
                if os.path.exists(label_path):
                    print(f"Fichier d√©j√† existant, ignor√© : {label_path}")
                    continue

                if not os.path.exists(parquet_path):
                    print(f"Fichier bookTicker introuvable : {parquet_path}")
                    continue

                print(f"Construction des labels pour : {symbol} {year_str}-{month_str}")

                # Chargement des booktickers
                df = pd.read_parquet(parquet_path)
                if self.light:
                    df.columns = [
                        "best_bid_price",
                        "best_ask_price",
                        "transaction_time",
                        ]
                else:
                    df.columns = [
                        "update_id",
                        "best_bid_price",
                        "best_bid_qty",
                        "best_ask_price",
                        "best_ask_qty",
                        "transaction_time",
                        "event_time"
                        ]

                # si transaction_time n'est pas au format date, conversion
                if not np.issubdtype(df["transaction_time"].dtype, np.datetime64):
                    try:
                        df["datetime"] = pd.to_datetime(df["transaction_time"].astype(np.int64), unit="ms")
                    except Exception as e:
                        print("Erreur conversion datetime:", e)
                        return []
                else:
                    df["datetime"] = df["transaction_time"]

                # R√©cup√©ration du jour
                df["day"] = df["datetime"].dt.day

                # Calcul du spread entre les s√©ries de bid / ask et normalisation
                df["spread"] = df["best_ask_price"] - df["best_bid_price"]
                df["spread"] = normalize(df["spread"])

                # Regroupement par jour ==> pas sur pour la normalisation, peut √™tre nul
                df_daily = df.groupby("day")["spread"].mean().reset_index()
                df_daily.columns = ["day", "spread_real"]

                # Si on travaille sur un mois √† moins de 30 jours, on passe (uniformisation des donn√©es) ==> √† revoir ; cas √† +30 jours g√©r√©s dans la m√©thode
                if df_daily.shape[0] < nb_days:
                    continue

                # Cas d'un mois > 30 jours : seuls les 30 premiers jours sont conserv√©s
                if df_daily.shape[0] > nb_days:
                    df_daily = df_daily.iloc[0:nb_days, :]

                # Export des donn√©es
                df_daily.to_parquet(label_path, index=False)
                print(f"Labels g√©n√©r√©s : {label_path}")
                label_paths.append(label_path)

                # Suppression du fichier contenant les booktickers pour all√©ger la m√©moire
                # os.remove(parquet_path)
                # print(f"Le fichier {parquet_path} a √©t√© supprim√© avec succ√®s")
        return label_paths

    def build_training_data(self, symbols=None, do_aggregate: bool = False):
        """
        M√©thode permettant de construire les dataframes d'entrainement

        output : format (nb_actif * nb_mois * 1440, nb_features) organis√©es avec les donn√©es pour chaque actif, mois par mois
        """

        if symbols is None:
            symbols = self.symbols

        # Liste pour stocker les r√©sultats successifs
        X_all = []
        y_all = []
        meta_all = []

        # Boucle sur les p√©riodes
        for year, month in self.dates:
            # Boucle sur les actifs
            for symbol in symbols:
                month_str = f"{month:02d}"
                year_str = str(year)
                base_name = f"{symbol}-1m-{year_str}-{month_str}"

                feature_path = os.path.join(
                    self.minute_features_data_dir,
                    f"{base_name}_features_1min_klines.parquet"
                )

                # V√©rification de l'existence des fichiers
                label_name = f"{symbol}-bookTicker-{year_str}-{month_str}_labels.parquet"
                label_path = os.path.join(self.labels_data_dir, label_name)

                if not os.path.exists(feature_path):
                    print(f"Fichier features manquant : {feature_path}")
                    continue
                if not os.path.exists(label_path):
                    print(f"Fichier labels manquant : {label_path}")
                    continue

                # Import des donn√©es
                df_feat = pd.read_parquet(feature_path)
                df_label = pd.read_parquet(label_path)

                # Agr√©gation de la s√©quence (cas du MLP, ...) ==> √† ajouter ici
                # df_feat = ...

                # Fusion des dataframes et r√©cup√©ration des features / spread
                df_merged = pd.merge(df_feat, df_label, on="day", how="inner")
                X = df_merged.loc[:, df_merged.columns != "spread_real"]
                y = df_merged["spread_real"]

                # Spread du jour = moyenne du spread intra-journalier
                # y = df_merged.groupby("day")["spread_real"].mean()

                meta_all.append(pd.DataFrame({
                    "symbol": [symbol] * len(df_merged),
                    "day": df_merged["day"],
                    "month":df_merged["month"]
                }))

                # Ajout dans la liste
                X_all.append(X)
                y_all.append(y)

        if not X_all:
            raise ValueError("Aucune donn√©e disponible pour entra√Æner le mod√®le.")

        # Concat√©nation des dataframes et conversion en array pour les traitements ult√©rieurs
        X_final = pd.concat(X_all, ignore_index=True).to_numpy(dtype=np.float32)
        y_final = pd.concat(y_all, ignore_index=True).to_numpy(dtype=np.float32)
        self.meta = pd.concat(meta_all, ignore_index=True)

        max_float32 = np.finfo(np.float32).max
        min_float32 = np.finfo(np.float32).min
        X_final = np.nan_to_num(X_final, nan=0.0, posinf=max_float32, neginf=min_float32)
        y_final = np.nan_to_num(y_final, nan=0.0, posinf=max_float32, neginf=min_float32)

        max_float32 = np.finfo(np.float32).max
        min_float32 = np.finfo(np.float32).min
        X_final = np.clip(X_final, min_float32, max_float32)
        y_final = np.clip(y_final, min_float32, max_float32)

        print(f"Donn√©es pr√™tes : X.shape = {X_final.shape}, y.shape = {y_final.shape}")
        return X_final, y_final

    def build_train_val_dataset(self, val_size = 0.2, is_test: bool = False, do_aggregate: bool = False):
        """
        M√©thode permettant de construire les dataframe d'input / output requis pour les mod√®les

        output :
        - un dataframe contenant les features pour tous les actifs et toutes les dates
        """

        # R√©cup√©ration des arrays avec les features/label pour tous les actifs date par date
        X, y = self.build_training_data(do_aggregate = do_aggregate)

        # R√©cup√©ration du nombre d'actifs du datamanager et de la longueur d'une s√©quence
        nb_assets: int = len(self.symbols)
        len_sequence: int = 1440

        # Cas o√π l'on travaille sur l'ensemble de test
        if is_test:

            # V√©rification : le nombre de ligne doit √™tre divisible par nb_assets * 1440 pour continuer les traitements
            while X.shape[0]%(nb_assets * len_sequence)!= 0:
                X = X[:-1,:]
            while y.shape[0]%nb_assets != 0:
                y = y[:-1,:]

            # transformation de y en spread moyen quotidien
            y = self.reduce_labels(y, len_sequence)
            return X,y

        # Autre cas : train / val split usuel
        else:
            if X.shape[0]%(nb_assets * len_sequence)!= 0:
                raise Exception("Probl√®me pour effectuer le split entre train et test")
            # S√©paration entre train / val et r√©cup√©ration
            X_train, y_train, X_val, y_val = self._time_series_split_minute(X, y, val_size)
        return X_train, X_val, y_train, y_val

    def time_series_features(self, X, y, daily = True, test_size=0.2, val_size=0.2):
        """
        Trie les donn√©es dans l'ordre [day, symbol] et applique un split temporel train / val / test.
        """
        if not daily:
            return self._time_series_split_minute(X, y, test_size, val_size)
        else:
            return self._time_series_split_daily(X, y, test_size, val_size)

    @staticmethod
    def reduce_labels(y_part, n_min: int):
        """
        fonction  permettant de passer des spread intraday au spread journalier moyen

        arguments :
        - nb_rows_per_day : nombre de lignes par mois
        """

        # Modification de la dimension de y : passage en 3D (nb_days * nb_months * nb_assets, 1440,1)
        spread_per_day = y_part.reshape(int(np.ceil(y_part.shape[0] / n_min)), n_min, 1)

        # Calcul de la moyenne intraday
        avg_daily_spread = np.mean(spread_per_day, axis=1)

        return avg_daily_spread

    def _time_series_split_minute(self, X, y, val_size):
        df_meta = self.meta.copy()
        df_meta["row_idx"] = np.arange(len(df_meta))

        df_meta_sorted = df_meta.sort_values(by=["day", "symbol"]).reset_index(drop=True)
        sorted_indices = df_meta_sorted["row_idx"].values

        X_sorted = X[sorted_indices]
        y_sorted = y[sorted_indices]
        days_sorted = df_meta_sorted["day"].values
        symbols_sorted = df_meta_sorted["symbol"].values

        # Nombre de minutes par jour
        n_min: int = 1440

        # R√©cup√©ration du nombre de jours / mois
        unique_days = np.unique(days_sorted)
        n_days = len(unique_days)

        # R√©cup√©ration du nombre de mois
        n_months: int = len(self.dates)

        # R√©cup√©ration du nombre de p√©riodes
        n_periods: int = n_months * n_days

        # Nombre de lignes par jours / Nombre de lignes dans le train
        rows_per_day = self.nb_assets * n_min
        nb_val_train: int = rows_per_day * n_days * n_months
        assert len(X_sorted) == nb_val_train, "X incoh√©rent avec nb_assets et minute-level"

        # Nombre de donn√©es temporelles √† conserver pour l'ensemble de train et de validation
        n_val = int(np.ceil(val_size * n_periods))
        n_train = n_periods - n_val

        def get_day_indices(start_day_idx, n_days_split):
            day_indices = []
            for i in range(start_day_idx, start_day_idx + n_days_split):
                start = i * rows_per_day
                end = (i + 1) * rows_per_day
                day_indices.append(np.arange(start, end))
            return np.concatenate(day_indices)

        idx_train = get_day_indices(0, n_train)
        idx_val   = get_day_indices(n_train, n_val)

        X_train = X_sorted[idx_train]
        X_val = X_sorted[idx_val]

        y_train_full = y_sorted[idx_train]
        y_val_full = y_sorted[idx_val]

        y_train = self.reduce_labels(y_train_full, n_min=n_min)
        y_val = self.reduce_labels(y_val_full, n_min=n_min)

        print(f"Split : train={len(X_train)}, val={len(X_val)}")
        print(f"Labels : y_train={len(y_train)}, y_val={len(y_val)}")

        return X_train, y_train, X_val, y_val

    def _time_series_split_daily(self, X, y, val_size):
        df_meta = self.meta.copy()
        df_meta["row_idx"] = np.arange(len(df_meta))
        df_meta_sorted = df_meta.sort_values(by=["day", "symbol"]).reset_index(drop=True)

        sorted_indices = df_meta_sorted["row_idx"].values
        X_sorted = X[sorted_indices]
        y_sorted = y[sorted_indices]
        days_sorted = df_meta_sorted["day"].values

        unique_days = np.unique(days_sorted)

        # D√©coupage du dataframe
        n_days = len(unique_days)
        n_val = int(np.ceil(val_size * n_days)) if val_size < 1 else int(val_size)
        n_train = n_days - n_val

        train_days = unique_days[:n_train]
        val_days = unique_days[n_train:n_train + n_val]

        def get_mask(days_subset):
            return np.isin(days_sorted, days_subset)

        mask_train = get_mask(train_days)
        mask_val = get_mask(val_days)

        X_train = X_sorted[mask_train]
        X_val = X_sorted[mask_val]

        y_train = y_sorted[mask_train]
        y_val = y_sorted[mask_val]

        print(f"Split : train={len(X_train)}, val={len(X_val)}")
        print(f"Labels : y_train={len(y_train)}, y_val={len(y_val)}")
        return X_train, y_train, X_val, y_val

    @staticmethod
    def make_batches(X_rnn, y_rnn, batch_size=32, shuffle=True):
        dataset = tf.data.Dataset.from_tensor_slices((X_rnn, y_rnn))
        if shuffle:
            dataset = dataset.shuffle(buffer_size=len(X_rnn))
        dataset = dataset.batch(batch_size)
        return dataset
    
    @staticmethod
    def format_data(
        X,
        y,
        model_type,
        daily=True,
        nb_assets=None,
        minutes_per_day=1440,
        window=None
    ):
        if nb_assets is None:
            raise ValueError("nb_assets doit √™tre sp√©cifi√©")

        model_type = model_type.lower()
        N, d = X.shape

        if daily:
            nb_days = N
            if y.shape[0] != nb_days:
                raise ValueError(f"[daily=True] y.shape[0] doit √™tre √©gal √† X.shape[0] (= {nb_days})")
            y_out = y
            rows_per_day = 1

        else:
            rows_per_day = nb_assets * minutes_per_day
            if N % rows_per_day != 0:
                raise ValueError(f"[daily=False] X.shape[0]={N} n'est pas divisible par nb_assets*minutes_per_day={rows_per_day}")
            nb_days = N // rows_per_day

            if y.shape[0] != nb_days * nb_assets:
                raise ValueError(f"[daily=False] y.shape[0]={y.shape[0]} attendu = nb_days*nb_assets = {nb_days}*{nb_assets}")
            y_out = y.reshape(nb_days, nb_assets)

        if model_type == "mlp":
            X_out = X.reshape(nb_days, rows_per_day * d)

        elif model_type == "cnn":
            X_out = X.reshape(nb_days, rows_per_day, d)

        elif model_type == "rnn":
            X_out = X.reshape(nb_days, rows_per_day, d)

        elif model_type == "seq":
            if window is None:
                raise ValueError("window doit √™tre sp√©cifi√© pour model_type='seq'")

            X_r = X.reshape(nb_days, rows_per_day, d)

            if nb_days <= window:
                return np.empty((0, window, rows_per_day * d), dtype=np.float32), np.empty((0, nb_assets), dtype=np.float32)

            X_out = np.array([
                X_r[t:t + window].reshape(window, -1)
                for t in range(nb_days - window)
            ], dtype=np.float32)

            y_out = y_out[window:] 

        else:
            raise ValueError(f"Mod√®le inconnu : '{model_type}'")

        return X_out.astype(np.float32), y_out.astype(np.float32)

    def compute_and_save_parametric_estimators(
            self, 
            X, 
            use_opposed=False, 
            sort_mode="asset_first"
            ):
        if sort_mode not in {"asset_first", "day_first"}:
            raise ValueError("sort_mode doit √™tre 'asset_first' ou 'day_first'.")

        crypto_names = sorted(self.symbols)
        crypto_str = "-".join(crypto_names)
        filename = f"param_est__{crypto_str}_{self.year:04d}_{self.month:02d}_" \
                f"opposed={use_opposed}_{sort_mode}.parquet"
        file_path = os.path.join(self.parametric_estimators_dir, filename)

        if os.path.exists(file_path):
            print(f"‚úÖ Fichier d√©j√† existant, pas de recalcul : {file_path}")
            return pd.read_parquet(file_path)

        df = pd.DataFrame(X, columns=["day", "open", "high", "low", "close", "volume"])

        meta_subset = self.generate_meta_from_X(X, sort_mode=sort_mode)
        list_dfs = []

        for asset in self.symbols:
            print(f"üìä Traitement de l'actif {asset}...")

            asset_mask = meta_subset["symbol"] == asset
            meta_asset = meta_subset[asset_mask]
            df_asset = df.loc[asset_mask.values]

            if df_asset.empty:
                print(f"‚ö†Ô∏è Aucun point trouv√© pour {asset}, on passe.")
                continue

            df_result = get_parametric_estimators_series(df_asset.values, meta_asset, use_opposed)
            list_dfs.append(df_result)

        if not list_dfs:
            raise ValueError("Aucune donn√©e valide trouv√©e dans X.")

        df_concat = pd.concat(list_dfs, axis=0, ignore_index=True)
        os.makedirs(self.parametric_estimators_dir, exist_ok=True)
        df_concat.to_parquet(file_path, index=False)
        print(f"‚úÖ Estimations sauvegard√©es dans : {file_path}")
        return df_concat

    def generate_meta_from_X(self, X, sort_mode="asset_first", minutes_per_day=1440):
        nb_assets = len(self.symbols)
        nb_rows = X.shape[0]
        total_minutes_per_day = minutes_per_day * nb_assets

        if nb_rows % total_minutes_per_day != 0:
            raise ValueError("X ne couvre pas un nombre entier de jours pour tous les actifs.")

        nb_days = nb_rows // total_minutes_per_day

        meta = []
        if sort_mode == "asset_first":
            for symbol in self.symbols:
                for day in range(nb_days):
                    meta.extend([{"symbol": symbol, "day": day}] * minutes_per_day)
        elif sort_mode == "day_first":
            for day in range(nb_days):
                for symbol in self.symbols:
                    meta.extend([{"symbol": symbol, "day": day}] * minutes_per_day)
        else:
            raise ValueError("sort_mode doit √™tre 'asset_first' ou 'day_first'.")

        return pd.DataFrame(meta)
